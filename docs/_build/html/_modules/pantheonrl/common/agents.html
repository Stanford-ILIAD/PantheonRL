<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>pantheonrl.common.agents &mdash; PantheonRL 0.1 documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../_static/documentation_options.js?v=2709fde1"></script>
        <script src="../../../_static/doctools.js?v=888ff710"></script>
        <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            PantheonRL
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../guide/install.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../_autosummary/pantheonrl.html">pantheonrl</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">PantheonRL</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">pantheonrl.common.agents</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for pantheonrl.common.agents</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span>

<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">th</span>

<span class="kn">from</span> <span class="nn">.util</span> <span class="kn">import</span> <span class="n">action_from_policy</span><span class="p">,</span> <span class="n">clip_actions</span><span class="p">,</span> <span class="n">resample_noise</span>
<span class="kn">from</span> <span class="nn">.trajsaver</span> <span class="kn">import</span> <span class="n">TransitionsMinimal</span>
<span class="kn">from</span> <span class="nn">.observation</span> <span class="kn">import</span> <span class="n">Observation</span>

<span class="kn">from</span> <span class="nn">stable_baselines3.common.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">configure_logger</span><span class="p">,</span>
    <span class="n">should_collect_more_steps</span><span class="p">,</span>
    <span class="n">obs_as_tensor</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">stable_baselines3.common.policies</span> <span class="kn">import</span> <span class="n">ActorCriticPolicy</span>
<span class="kn">from</span> <span class="nn">stable_baselines3.common.on_policy_algorithm</span> <span class="kn">import</span> <span class="n">OnPolicyAlgorithm</span>
<span class="kn">from</span> <span class="nn">stable_baselines3.common.off_policy_algorithm</span> <span class="kn">import</span> <span class="n">OffPolicyAlgorithm</span>
<span class="kn">from</span> <span class="nn">stable_baselines3.common.utils</span> <span class="kn">import</span> <span class="n">safe_mean</span>

<span class="kn">from</span> <span class="nn">threading</span> <span class="kn">import</span> <span class="n">Condition</span>

<span class="kn">from</span> <span class="nn">gymnasium</span> <span class="kn">import</span> <span class="n">spaces</span>

<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">sys</span>


<div class="viewcode-block" id="Agent">
<a class="viewcode-back" href="../../../_autosummary/pantheonrl.common.agents.Agent.html#pantheonrl.common.agents.Agent">[docs]</a>
<span class="k">class</span> <span class="nc">Agent</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for all agents in multi-agent environments</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="Agent.get_action">
<a class="viewcode-back" href="../../../_autosummary/pantheonrl.common.agents.Agent.html#pantheonrl.common.agents.Agent.get_action">[docs]</a>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="n">Observation</span><span class="p">,</span> <span class="n">record</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return an action given an observation.</span>

<span class="sd">        :param obs: The observation to use</span>
<span class="sd">        :param record: Whether to record the obs, action pair (for training)</span>
<span class="sd">        :returns: The action to take</span>
<span class="sd">        &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="Agent.update">
<a class="viewcode-back" href="../../../_autosummary/pantheonrl.common.agents.Agent.html#pantheonrl.common.agents.Agent.update">[docs]</a>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">done</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add new rewards and done information if the agent can learn.</span>

<span class="sd">        Each update corresponds to the most recent `get_action` (where</span>
<span class="sd">        `record` is True). If there are multiple calls to `update` that</span>
<span class="sd">        correspond to the same `get_action`, their rewards are summed up and</span>
<span class="sd">        the last done flag will be used.</span>

<span class="sd">        :param reward: The reward receieved from the previous action step</span>
<span class="sd">        :param done: Whether the game is done</span>
<span class="sd">        &quot;&quot;&quot;</span></div>
</div>



<div class="viewcode-block" id="DummyAgent">
<a class="viewcode-back" href="../../../_autosummary/pantheonrl.common.agents.DummyAgent.html#pantheonrl.common.agents.DummyAgent">[docs]</a>
<span class="k">class</span> <span class="nc">DummyAgent</span><span class="p">(</span><span class="n">Agent</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Agent wrapper for standard SARL algorithms assuming a gym interface</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dummy_env</span><span class="p">):</span>
        <span class="c1"># print(&quot;Constructing Dummy Agent&quot;)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rew</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dummy_env</span> <span class="o">=</span> <span class="n">dummy_env</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_action</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_cv</span> <span class="o">=</span> <span class="n">Condition</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dummy_env</span><span class="o">.</span><span class="n">associated_agent</span> <span class="o">=</span> <span class="bp">self</span>

<div class="viewcode-block" id="DummyAgent.get_action">
<a class="viewcode-back" href="../../../_autosummary/pantheonrl.common.agents.DummyAgent.html#pantheonrl.common.agents.DummyAgent.get_action">[docs]</a>
    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="n">Observation</span><span class="p">,</span> <span class="n">record</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="c1"># print(&quot;Dummy Agent: got new observation&quot;)</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">dummy_env</span><span class="o">.</span><span class="n">obs_cv</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dummy_env</span><span class="o">.</span><span class="n">_obs</span> <span class="o">=</span> <span class="n">obs</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dummy_env</span><span class="o">.</span><span class="n">_rew</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rew</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dummy_env</span><span class="o">.</span><span class="n">_done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">done</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rew</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="c1"># print(&quot;Dummy Agent: sent observation notification&quot;)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dummy_env</span><span class="o">.</span><span class="n">obs_cv</span><span class="o">.</span><span class="n">notify</span><span class="p">()</span>

        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_cv</span><span class="p">:</span>
            <span class="c1"># print(&quot;Dummy Agent: waiting for action&quot;)</span>
            <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">_action</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">action_cv</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
            <span class="n">to_return</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_action</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_action</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="c1"># print(&quot;Dummy Agent: got action&quot;)</span>
        <span class="k">return</span> <span class="n">to_return</span></div>


<div class="viewcode-block" id="DummyAgent.update">
<a class="viewcode-back" href="../../../_autosummary/pantheonrl.common.agents.DummyAgent.html#pantheonrl.common.agents.DummyAgent.update">[docs]</a>
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">done</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rew</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">done</span> <span class="ow">or</span> <span class="n">done</span></div>
</div>



<div class="viewcode-block" id="StaticPolicyAgent">
<a class="viewcode-back" href="../../../_autosummary/pantheonrl.common.agents.StaticPolicyAgent.html#pantheonrl.common.agents.StaticPolicyAgent">[docs]</a>
<span class="k">class</span> <span class="nc">StaticPolicyAgent</span><span class="p">(</span><span class="n">Agent</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Agent representing a static (not learning) policy.</span>

<span class="sd">    :param policy: Policy representing the agent&#39;s responses to observations</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policy</span><span class="p">:</span> <span class="n">ActorCriticPolicy</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy</span> <span class="o">=</span> <span class="n">policy</span>

<div class="viewcode-block" id="StaticPolicyAgent.get_action">
<a class="viewcode-back" href="../../../_autosummary/pantheonrl.common.agents.StaticPolicyAgent.html#pantheonrl.common.agents.StaticPolicyAgent.get_action">[docs]</a>
    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="n">Observation</span><span class="p">,</span> <span class="n">record</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return an action given an observation.</span>

<span class="sd">        :param obs: The observation to use</span>
<span class="sd">        :param record: Whether to record the obs, action (unused)</span>
<span class="sd">        :returns: The action to take</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">actions</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">action_from_policy</span><span class="p">(</span><span class="n">obs</span><span class="o">.</span><span class="n">obs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">clip_actions</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="StaticPolicyAgent.update">
<a class="viewcode-back" href="../../../_autosummary/pantheonrl.common.agents.StaticPolicyAgent.html#pantheonrl.common.agents.StaticPolicyAgent.update">[docs]</a>
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">done</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Update does nothing since the agent does not learn.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div>
</div>



<div class="viewcode-block" id="OnPolicyAgent">
<a class="viewcode-back" href="../../../_autosummary/pantheonrl.common.agents.OnPolicyAgent.html#pantheonrl.common.agents.OnPolicyAgent">[docs]</a>
<span class="k">class</span> <span class="nc">OnPolicyAgent</span><span class="p">(</span><span class="n">Agent</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Agent representing an on-policy learning algorithm (ex: A2C/PPO).</span>

<span class="sd">    The `get_action` and `update` functions are based on the `learn` function</span>
<span class="sd">    from ``OnPolicyAlgorithm``.</span>

<span class="sd">    :param model: Model representing the agent&#39;s learning algorithm</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">model</span><span class="p">:</span> <span class="n">OnPolicyAlgorithm</span><span class="p">,</span>
                 <span class="n">log_interval</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">tensorboard_log</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">working_timesteps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                 <span class="n">callback</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">tb_log_name</span><span class="o">=</span><span class="s2">&quot;OnPolicyAgent&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tb_log_name</span> <span class="o">=</span> <span class="n">tb_log_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">original_callback</span> <span class="o">=</span> <span class="n">callback</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_last_obs</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_last_episode_starts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">working_timesteps</span> <span class="o">=</span> <span class="n">working_timesteps</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">total_timesteps</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_setup_learn</span><span class="p">(</span>
            <span class="n">working_timesteps</span><span class="p">,</span>
            <span class="n">callback</span><span class="p">,</span>
            <span class="kc">False</span><span class="p">,</span>
            <span class="n">tb_log_name</span><span class="p">,</span>
            <span class="kc">False</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_training_start</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="nb">globals</span><span class="p">())</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">set_training_mode</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">rollout_buffer</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">use_sde</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">reset_noise</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_rollout_start</span><span class="p">()</span>

        <span class="c1"># self.values: th.Tensor = th.empty(0)</span>

        <span class="c1"># self.model.set_logger(configure_logger(</span>
        <span class="c1">#     self.model.verbose, tensorboard_log, tb_log_name))</span>

        <span class="c1"># self.name = tb_log_name</span>
        <span class="c1"># self.num_timesteps = 0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_interval</span> <span class="o">=</span> <span class="n">log_interval</span> <span class="ow">or</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">verbose</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>
        <span class="c1"># self.iteration = 0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">ep_info_buffer</span> <span class="o">=</span> <span class="n">deque</span><span class="p">([],</span> <span class="n">maxlen</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_progress_info</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;r&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;l&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>

        <span class="c1"># self.model.policy.set_training_mode(False)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">old_buffer</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="OnPolicyAgent.get_action">
<a class="viewcode-back" href="../../../_autosummary/pantheonrl.common.agents.OnPolicyAgent.html#pantheonrl.common.agents.OnPolicyAgent.get_action">[docs]</a>
    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="n">Observation</span><span class="p">,</span> <span class="n">record</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return an action given an observation.</span>

<span class="sd">        When `record` is True, the agent saves the last transition into its</span>
<span class="sd">        buffer. It also updates the model if the buffer is full.</span>

<span class="sd">        :param obs: The observation to use</span>
<span class="sd">        :param record: Whether to record the obs, action (True when training)</span>
<span class="sd">        :returns: The action to take</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="o">.</span><span class="n">obs</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">obs</span><span class="p">])</span>
        <span class="n">callback</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback</span>
        <span class="n">rollout_buffer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">rollout_buffer</span>
        <span class="n">n_rollout_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">n_steps</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">num_timesteps</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_timesteps</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_training_end</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">total_timesteps</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_setup_learn</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">working_timesteps</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">original_callback</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tb_log_name</span><span class="p">,</span>
                <span class="kc">False</span>
            <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_training_start</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="nb">globals</span><span class="p">())</span>

        <span class="k">if</span> <span class="n">record</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">&gt;=</span> <span class="n">n_rollout_steps</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">th</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">predict_values</span><span class="p">(</span><span class="n">obs_as_tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
            <span class="n">rollout_buffer</span><span class="o">.</span><span class="n">compute_returns_and_advantage</span><span class="p">(</span><span class="n">last_values</span><span class="o">=</span><span class="n">values</span><span class="p">,</span> <span class="n">dones</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_last_episode_starts</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">old_buffer</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">rollout_buffer</span><span class="p">)</span>
            <span class="n">callback</span><span class="o">.</span><span class="n">update_locals</span><span class="p">(</span><span class="nb">locals</span><span class="p">())</span>
            <span class="n">callback</span><span class="o">.</span><span class="n">on_rollout_end</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_update_current_progress_remaining</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">num_timesteps</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">working_timesteps</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_interval</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">ep_info_buffer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="c1"># TODO, Logging</span>
                <span class="n">time_elapsed</span> <span class="o">=</span> <span class="nb">max</span><span class="p">((</span><span class="n">time</span><span class="o">.</span><span class="n">time_ns</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">start_time</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1e9</span><span class="p">,</span> <span class="n">sys</span><span class="o">.</span><span class="n">float_info</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>
                <span class="n">fps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">num_timesteps</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_num_timesteps_at_start</span><span class="p">)</span> <span class="o">/</span> <span class="n">time_elapsed</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="s2">&quot;time/iterations&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="p">,</span> <span class="n">exclude</span><span class="o">=</span><span class="s2">&quot;tensorboard&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">ep_info_buffer</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">ep_info_buffer</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="s2">&quot;rollout/ep_rew_mean&quot;</span><span class="p">,</span> <span class="n">safe_mean</span><span class="p">([</span><span class="n">ep_info</span><span class="p">[</span><span class="s2">&quot;r&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">ep_info</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">ep_info_buffer</span><span class="p">]))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="s2">&quot;rollout/ep_len_mean&quot;</span><span class="p">,</span> <span class="n">safe_mean</span><span class="p">([</span><span class="n">ep_info</span><span class="p">[</span><span class="s2">&quot;l&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">ep_info</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">ep_info_buffer</span><span class="p">]))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="s2">&quot;time/fps&quot;</span><span class="p">,</span> <span class="n">fps</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="s2">&quot;time/time_elapsed&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">time_elapsed</span><span class="p">),</span> <span class="n">exclude</span><span class="o">=</span><span class="s2">&quot;tensorboard&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="s2">&quot;time/total_timesteps&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">num_timesteps</span><span class="p">,</span> <span class="n">exclude</span><span class="o">=</span><span class="s2">&quot;tensorboard&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">num_timesteps</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

            <span class="c1"># Restarting</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">set_training_mode</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">rollout_buffer</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">use_sde</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">reset_noise</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_rollout_start</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">use_sde</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">sde_sample_freq</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">sde_sample_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">reset_noise</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">th</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">obs_tensor</span> <span class="o">=</span> <span class="n">obs_as_tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">actions</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">log_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">policy</span><span class="p">(</span><span class="n">obs_tensor</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="n">actions</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">clipped_actions</span> <span class="o">=</span> <span class="n">actions</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span> <span class="n">spaces</span><span class="o">.</span><span class="n">Box</span><span class="p">):</span>
            <span class="n">clipped_actions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">low</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">high</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">record</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">in_progress_info</span><span class="p">[</span><span class="s2">&quot;l&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">num_timesteps</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span> <span class="n">spaces</span><span class="o">.</span><span class="n">Discrete</span><span class="p">):</span>
            <span class="n">actions</span> <span class="o">=</span> <span class="n">actions</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">rollout_buffer</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
            <span class="n">obs</span><span class="p">,</span>
            <span class="n">actions</span><span class="p">,</span>
            <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_last_episode_starts</span><span class="p">,</span>
            <span class="n">values</span><span class="p">,</span>
            <span class="n">log_probs</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">clipped_actions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="OnPolicyAgent.update">
<a class="viewcode-back" href="../../../_autosummary/pantheonrl.common.agents.OnPolicyAgent.html#pantheonrl.common.agents.OnPolicyAgent.update">[docs]</a>
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">done</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add new rewards and done information.</span>

<span class="sd">        The rewards are added to buffer entry corresponding to the most recent</span>
<span class="sd">        recorded action.</span>

<span class="sd">        :param reward: The reward receieved from the previous action step</span>
<span class="sd">        :param done: Whether the game is done</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">buf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">rollout_buffer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_last_episode_starts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">done</span>
        <span class="n">buf</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="n">buf</span><span class="o">.</span><span class="n">pos</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_progress_info</span><span class="p">[</span><span class="s2">&quot;r&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">ep_info_buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_progress_info</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">in_progress_info</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;r&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;l&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span></div>


<div class="viewcode-block" id="OnPolicyAgent.learn">
<a class="viewcode-back" href="../../../_autosummary/pantheonrl.common.agents.OnPolicyAgent.html#pantheonrl.common.agents.OnPolicyAgent.learn">[docs]</a>
    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Call the model&#39;s learn function with the given parameters &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_custom_logger</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>
</div>



<div class="viewcode-block" id="OffPolicyAgent">
<a class="viewcode-back" href="../../../_autosummary/pantheonrl.common.agents.OffPolicyAgent.html#pantheonrl.common.agents.OffPolicyAgent">[docs]</a>
<span class="k">class</span> <span class="nc">OffPolicyAgent</span><span class="p">(</span><span class="n">Agent</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Agent representing an off-policy learning algorithm (ex: DQN/SAC).</span>

<span class="sd">    The `get_action` and `update` functions are based on the `learn` function</span>
<span class="sd">    from ``OffPolicyAlgorithm``.</span>

<span class="sd">    :param model: Model representing the agent&#39;s learning algorithm</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">model</span><span class="p">:</span> <span class="n">OffPolicyAlgorithm</span><span class="p">,</span>
                 <span class="n">log_interval</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">tensorboard_log</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">working_timesteps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                 <span class="n">callback</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">tb_log_name</span><span class="o">=</span><span class="s2">&quot;OffPolicyAgent&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tb_log_name</span> <span class="o">=</span> <span class="n">tb_log_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">original_callback</span> <span class="o">=</span> <span class="n">callback</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_interval</span> <span class="o">=</span> <span class="n">log_interval</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_last_obs</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_last_episode_starts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">working_timesteps</span> <span class="o">=</span> <span class="n">working_timesteps</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">total_timesteps</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_setup_learn</span><span class="p">(</span>
            <span class="n">working_timesteps</span><span class="p">,</span>
            <span class="n">callback</span><span class="p">,</span>
            <span class="kc">False</span><span class="p">,</span>
            <span class="n">tb_log_name</span><span class="p">,</span>
            <span class="kc">False</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_training_start</span><span class="p">(</span><span class="nb">locals</span><span class="p">(),</span> <span class="nb">globals</span><span class="p">())</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">set_training_mode</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_collected_steps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_collected_episodes</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">use_sde</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">reset_noise</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_rollout_start</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">buffer_actions</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.</span><span class="p">]</span> <span class="c1">#np.zeros((1,))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dones</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="c1">#np.zeros((1,), dtype=bool)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">infos</span> <span class="o">=</span> <span class="p">[{}]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_interval</span> <span class="o">=</span> <span class="n">log_interval</span> <span class="ow">or</span> <span class="p">(</span><span class="mi">4</span> <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">verbose</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cur_ep_info</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;l&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>

<div class="viewcode-block" id="OffPolicyAgent.get_action">
<a class="viewcode-back" href="../../../_autosummary/pantheonrl.common.agents.OffPolicyAgent.html#pantheonrl.common.agents.OffPolicyAgent.get_action">[docs]</a>
    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="n">Observation</span><span class="p">,</span> <span class="n">record</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return an action given an observation.</span>

<span class="sd">        When `record` is True, the agent saves the last transition into its</span>
<span class="sd">        buffer.</span>

<span class="sd">        :param obs: The observation to use</span>
<span class="sd">        :param record: Whether to record the obs, action (True when training)</span>
<span class="sd">        :returns: The action to take</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="o">.</span><span class="n">obs</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">obs</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">callback</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback</span>
        <span class="n">train_freq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train_freq</span>
        <span class="n">replay_buffer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">replay_buffer</span>
        <span class="n">action_noise</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">action_noise</span>
        <span class="n">learning_starts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">learning_starts</span>
        <span class="n">log_interval</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_interval</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_actions</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_last_obs</span> <span class="o">=</span> <span class="n">obs</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_obs</span> <span class="o">=</span> <span class="n">obs</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_update_info_buffer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">infos</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dones</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_store_transition</span><span class="p">(</span><span class="n">replay_buffer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">new_obs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dones</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">infos</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_update_current_progress_remaining</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">num_timesteps</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_total_timesteps</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_on_step</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">done</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dones</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">num_collected_episodes</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_episode_num</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="k">if</span> <span class="n">action_noise</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">action_noise</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">log_interval</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_episode_num</span> <span class="o">%</span> <span class="n">log_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_dump_logs</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">should_collect_more_steps</span><span class="p">(</span><span class="n">train_freq</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_collected_steps</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_collected_episodes</span><span class="p">):</span>
            <span class="n">callback</span><span class="o">.</span><span class="n">on_rollout_end</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">num_timesteps</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">num_timesteps</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">learning_starts</span><span class="p">:</span>
                <span class="n">gradient_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">gradient_steps</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">gradient_steps</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_collected_steps</span>
                <span class="k">if</span> <span class="n">gradient_steps</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">gradient_steps</span><span class="o">=</span><span class="n">gradient_steps</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">set_training_mode</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_collected_steps</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_collected_episodes</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">use_sde</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">reset_noise</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">callback</span><span class="o">.</span><span class="n">on_rollout_start</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">use_sde</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">sde_sample_freq</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_collected_steps</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">sde_sample_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">actor</span><span class="o">.</span><span class="n">reset_noise</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">actions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_sample_action</span><span class="p">(</span><span class="n">learning_starts</span><span class="p">,</span> <span class="n">action_noise</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">num_timesteps</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_collected_steps</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dones</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">infos</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cur_ep_info</span><span class="p">[</span><span class="s1">&#39;l&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">actions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="OffPolicyAgent.update">
<a class="viewcode-back" href="../../../_autosummary/pantheonrl.common.agents.OffPolicyAgent.html#pantheonrl.common.agents.OffPolicyAgent.update">[docs]</a>
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">done</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add new rewards and done information.</span>

<span class="sd">        The agent trains when the model determines that it has collected enough</span>
<span class="sd">        timesteps.</span>

<span class="sd">        :param reward: The reward receieved from the previous action step</span>
<span class="sd">        :param done: Whether the game is done</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dones</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">done</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">infos</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cur_ep_info</span><span class="p">[</span><span class="s1">&#39;r&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">infos</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;episode&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cur_ep_info</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cur_ep_info</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;l&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span></div>


    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_custom_logger</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>



<div class="viewcode-block" id="RecordingAgentWrapper">
<a class="viewcode-back" href="../../../_autosummary/pantheonrl.common.agents.RecordingAgentWrapper.html#pantheonrl.common.agents.RecordingAgentWrapper">[docs]</a>
<span class="k">class</span> <span class="nc">RecordingAgentWrapper</span><span class="p">(</span><span class="n">Agent</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Wrapper for an agent that records observation-action pairs.</span>

<span class="sd">    Users can also use SimultaneousRecorder or TurnBasedRecorder (from</span>
<span class="sd">    wrappers.py) to record the transitions in an environment.</span>

<span class="sd">    :param realagent: Agent that defines the behaviour of this actor</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">realagent</span><span class="p">:</span> <span class="n">Agent</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">realagent</span> <span class="o">=</span> <span class="n">realagent</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">allobs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">allacts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

<div class="viewcode-block" id="RecordingAgentWrapper.get_action">
<a class="viewcode-back" href="../../../_autosummary/pantheonrl.common.agents.RecordingAgentWrapper.html#pantheonrl.common.agents.RecordingAgentWrapper.get_action">[docs]</a>
    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="n">Observation</span><span class="p">,</span> <span class="n">record</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return an action given an observation.</span>

<span class="sd">        The output is the same as calling `get_action` on the realagent, but</span>
<span class="sd">        this wrapper also stores the observation-action pair to a buffer</span>

<span class="sd">        :param obs: The observation to use</span>
<span class="sd">        :param record: Whether to record the obs, action (True when training)</span>
<span class="sd">        :returns: The action to take</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">realagent</span><span class="o">.</span><span class="n">get_action</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">record</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">allobs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">obs</span><span class="o">.</span><span class="n">obs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">allacts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">action</span></div>


<div class="viewcode-block" id="RecordingAgentWrapper.update">
<a class="viewcode-back" href="../../../_autosummary/pantheonrl.common.agents.RecordingAgentWrapper.html#pantheonrl.common.agents.RecordingAgentWrapper.update">[docs]</a>
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">done</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Simply calls the realagent&#39;s update function</span>

<span class="sd">        :param reward: The reward receieved from the previous action step</span>
<span class="sd">        :param done: Whether the game is done</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">realagent</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span></div>


<div class="viewcode-block" id="RecordingAgentWrapper.get_transitions">
<a class="viewcode-back" href="../../../_autosummary/pantheonrl.common.agents.RecordingAgentWrapper.html#pantheonrl.common.agents.RecordingAgentWrapper.get_transitions">[docs]</a>
    <span class="k">def</span> <span class="nf">get_transitions</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TransitionsMinimal</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the transitions recorded by this agent.</span>

<span class="sd">        :returns: A TransitionsMinimal object representing the transitions</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">allobs</span><span class="p">)</span>
        <span class="n">acts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">allacts</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">TransitionsMinimal</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">acts</span><span class="p">)</span></div>
</div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Stanford ILIAD.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>