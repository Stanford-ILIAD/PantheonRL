<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>pantheonrl.algos.adap.adap_learn.ADAP &mdash; PantheonRL 0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=2709fde1"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="pantheonrl.algos.adap.agent" href="pantheonrl.algos.adap.agent.html" />
    <link rel="prev" title="pantheonrl.algos.adap.adap_learn" href="pantheonrl.algos.adap.adap_learn.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            PantheonRL
              <img src="../_static/Pantheon.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/install.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API reference</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="pantheonrl.html">pantheonrl</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="pantheonrl.algos.html">pantheonrl.algos</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="pantheonrl.algos.adap.html">pantheonrl.algos.adap</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="pantheonrl.algos.adap.adap_learn.html">pantheonrl.algos.adap.adap_learn</a></li>
<li class="toctree-l4"><a class="reference internal" href="pantheonrl.algos.adap.agent.html">pantheonrl.algos.adap.agent</a></li>
<li class="toctree-l4"><a class="reference internal" href="pantheonrl.algos.adap.policies.html">pantheonrl.algos.adap.policies</a></li>
<li class="toctree-l4"><a class="reference internal" href="pantheonrl.algos.adap.util.html">pantheonrl.algos.adap.util</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="pantheonrl.algos.bc.html">pantheonrl.algos.bc</a></li>
<li class="toctree-l3"><a class="reference internal" href="pantheonrl.algos.modular.html">pantheonrl.algos.modular</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="pantheonrl.common.html">pantheonrl.common</a></li>
<li class="toctree-l2"><a class="reference internal" href="pantheonrl.envs.html">pantheonrl.envs</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PantheonRL</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="pantheonrl.html">pantheonrl</a></li>
          <li class="breadcrumb-item"><a href="pantheonrl.algos.html">pantheonrl.algos</a></li>
          <li class="breadcrumb-item"><a href="pantheonrl.algos.adap.html">pantheonrl.algos.adap</a></li>
          <li class="breadcrumb-item"><a href="pantheonrl.algos.adap.adap_learn.html">pantheonrl.algos.adap.adap_learn</a></li>
      <li class="breadcrumb-item active">pantheonrl.algos.adap.adap_learn.ADAP</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="pantheonrl-algos-adap-adap-learn-adap">
<h1>pantheonrl.algos.adap.adap_learn.ADAP<a class="headerlink" href="#pantheonrl-algos-adap-adap-learn-adap" title="Link to this heading"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="pantheonrl.algos.adap.adap_learn.ADAP">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">ADAP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">policy</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0003</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gae_lambda</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.95</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_range_vf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ent_coef</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vf_coef</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_grad_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_sde</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sde_sample_freq</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_kl</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensorboard_log</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">create_eval_env</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_init_setup_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context_loss_coeff</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_context_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context_sampler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'l2'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_state_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pantheonrl/algos/adap/adap_learn.html#ADAP"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pantheonrl.algos.adap.adap_learn.ADAP" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">OnPolicyAlgorithm</span></code></p>
<p>Borrows from Proximal Policy Optimization algorithm (PPO) (clip version)
Paper: <a class="reference external" href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a>
Code: This implementation borrows code from OpenAI Spinning Up
(<a class="reference external" href="https://github.com/openai/spinningup/">https://github.com/openai/spinningup/</a>)
<a class="reference external" href="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail">https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail</a> and
and Stable Baselines (PPO2 from <a class="reference external" href="https://github.com/hill-a/stable-baselines">https://github.com/hill-a/stable-baselines</a>)
Introduction to PPO:
<a class="reference external" href="https://spinningup.openai.com/en/latest/algorithms/ppo.html">https://spinningup.openai.com/en/latest/algorithms/ppo.html</a>
:param policy: The policy model to use (MlpPolicy, CnnPolicy, …)
:param env: The environment to learn from</p>
<blockquote>
<div><p>(if registered in Gym, can be str)</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>learning_rate</strong> (<em>float</em><em> | </em><em>Callable</em><em>[</em><em>[</em><em>float</em><em>]</em><em>, </em><em>float</em><em>]</em>) – The learning rate, it can be a function
of the current progress remaining (from 1 to 0)</p></li>
<li><p><strong>n_steps</strong> (<em>int</em>) – The number of steps to run for each environment per update
(i.e. rollout buffer size is n_steps * n_envs where n_envs is number of
environment copies running in parallel)
NOTE: n_steps * n_envs must be greater than 1 (because of the advantage
normalization) See <a class="reference external" href="https://github.com/pytorch/pytorch/issues/29372">https://github.com/pytorch/pytorch/issues/29372</a></p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Minibatch size</p></li>
<li><p><strong>n_epochs</strong> (<em>int</em>) – Number of epoch when optimizing the surrogate loss</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – Discount factor</p></li>
<li><p><strong>gae_lambda</strong> (<em>float</em>) – Factor for trade-off of bias vs variance for Generalized
Advantage Estimator</p></li>
<li><p><strong>clip_range</strong> (<em>float</em><em> | </em><em>Callable</em><em>[</em><em>[</em><em>float</em><em>]</em><em>, </em><em>float</em><em>]</em>) – Clipping parameter, it can be a function of the current
progress remaining (from 1 to 0).</p></li>
<li><p><strong>clip_range_vf</strong> (<em>None</em><em> | </em><em>float</em><em> | </em><em>Callable</em><em>[</em><em>[</em><em>float</em><em>]</em><em>, </em><em>float</em><em>]</em>) – Clipping parameter for the value function,
it can be a function of the current progress remaining (from 1 to 0).
This is a parameter specific to the OpenAI implementation. If None is
passed (default), no clipping will be done on the value function.
IMPORTANT: this clipping depends on the reward scaling.</p></li>
<li><p><strong>ent_coef</strong> (<em>float</em>) – Entropy coefficient for the loss calculation</p></li>
<li><p><strong>vf_coef</strong> (<em>float</em>) – Value function coefficient for the loss calculation</p></li>
<li><p><strong>max_grad_norm</strong> (<em>float</em>) – The maximum value for the gradient clipping</p></li>
<li><p><strong>use_sde</strong> (<em>bool</em>) – Whether to use generalized State Dependent Exploration
(gSDE) instead of action noise exploration (default: False)</p></li>
<li><p><strong>sde_sample_freq</strong> (<em>int</em>) – Sample a new noise matrix every n steps when using
gSDE
Default: -1 (only sample at the beginning of the rollout)</p></li>
<li><p><strong>target_kl</strong> (<em>float</em><em> | </em><em>None</em>) – Limit the KL divergence between updates,
because the clipping is not enough to prevent large update
see issue #213
(cf <a class="reference external" href="https://github.com/hill-a/stable-baselines/issues/213">https://github.com/hill-a/stable-baselines/issues/213</a>)
By default, there is no limit on the kl div.</p></li>
<li><p><strong>tensorboard_log</strong> (<em>str</em><em> | </em><em>None</em>) – the log location for tensorboard
(if None, no logging)</p></li>
<li><p><strong>create_eval_env</strong> (<em>bool</em>) – Whether to create a second environment that will be
used for evaluating the agent periodically. (Only available when
passing string for the environment)</p></li>
<li><p><strong>policy_kwargs</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>] </em><em>| </em><em>None</em>) – additional arguments to be passed to the policy on
creation</p></li>
<li><p><strong>verbose</strong> (<em>int</em>) – the verbosity level: 0 no output, 1 info, 2 debug</p></li>
<li><p><strong>seed</strong> (<em>int</em><em> | </em><em>None</em>) – Seed for the pseudo random generators</p></li>
<li><p><strong>device</strong> (<em>device</em><em> | </em><em>str</em>) – Device (cpu, cuda, …) on which the code should be run.
Setting it to auto, the code will be run on the GPU if possible.</p></li>
<li><p><strong>_init_setup_model</strong> (<em>bool</em>) – Whether or not to build the network at the
creation of the instance</p></li>
<li><p><strong>policy</strong> (<em>ActorCriticPolicy</em>) – </p></li>
<li><p><strong>env</strong> (<em>Env</em><em> | </em><em>VecEnv</em><em> | </em><em>str</em>) – </p></li>
<li><p><strong>context_loss_coeff</strong> (<em>float</em>) – </p></li>
<li><p><strong>context_size</strong> (<em>int</em>) – </p></li>
<li><p><strong>num_context_samples</strong> (<em>int</em>) – </p></li>
<li><p><strong>context_sampler</strong> (<em>str</em>) – </p></li>
<li><p><strong>num_state_samples</strong> (<em>int</em>) – </p></li>
</ul>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#pantheonrl.algos.adap.adap_learn.ADAP.collect_rollouts" title="pantheonrl.algos.adap.adap_learn.ADAP.collect_rollouts"><code class="xref py py-obj docutils literal notranslate"><span class="pre">collect_rollouts</span></code></a></p></td>
<td><p>Nearly identical to OnPolicyAlgorithm's collect_rollouts, but it also resamples the context every episode.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pantheonrl.algos.adap.adap_learn.ADAP.get_env" title="pantheonrl.algos.adap.adap_learn.ADAP.get_env"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_env</span></code></a></p></td>
<td><p>Returns the current environment (can be None if not defined).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pantheonrl.algos.adap.adap_learn.ADAP.get_parameters" title="pantheonrl.algos.adap.adap_learn.ADAP.get_parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_parameters</span></code></a></p></td>
<td><p>Return the parameters of the agent.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pantheonrl.algos.adap.adap_learn.ADAP.get_vec_normalize_env" title="pantheonrl.algos.adap.adap_learn.ADAP.get_vec_normalize_env"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_vec_normalize_env</span></code></a></p></td>
<td><p>Return the <code class="docutils literal notranslate"><span class="pre">VecNormalize</span></code> wrapper of the training env if it exists.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pantheonrl.algos.adap.adap_learn.ADAP.learn" title="pantheonrl.algos.adap.adap_learn.ADAP.learn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">learn</span></code></a></p></td>
<td><p>Return a trained model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pantheonrl.algos.adap.adap_learn.ADAP.load" title="pantheonrl.algos.adap.adap_learn.ADAP.load"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load</span></code></a></p></td>
<td><p>Load the model from a zip-file.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pantheonrl.algos.adap.adap_learn.ADAP.predict" title="pantheonrl.algos.adap.adap_learn.ADAP.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a></p></td>
<td><p>Get the policy action from an observation (and optional hidden state).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pantheonrl.algos.adap.adap_learn.ADAP.save" title="pantheonrl.algos.adap.adap_learn.ADAP.save"><code class="xref py py-obj docutils literal notranslate"><span class="pre">save</span></code></a></p></td>
<td><p>Save all the attributes of the object and the model parameters in a zip-file.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pantheonrl.algos.adap.adap_learn.ADAP.set_env" title="pantheonrl.algos.adap.adap_learn.ADAP.set_env"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_env</span></code></a></p></td>
<td><p>Checks the validity of the environment, and if it is coherent, set it as the current environment.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pantheonrl.algos.adap.adap_learn.ADAP.set_logger" title="pantheonrl.algos.adap.adap_learn.ADAP.set_logger"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_logger</span></code></a></p></td>
<td><p>Setter for for logger object.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pantheonrl.algos.adap.adap_learn.ADAP.set_parameters" title="pantheonrl.algos.adap.adap_learn.ADAP.set_parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_parameters</span></code></a></p></td>
<td><p>Load parameters from a given zip-file or a nested dictionary containing parameters for different modules (see <code class="docutils literal notranslate"><span class="pre">get_parameters</span></code>).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#pantheonrl.algos.adap.adap_learn.ADAP.set_random_seed" title="pantheonrl.algos.adap.adap_learn.ADAP.set_random_seed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_random_seed</span></code></a></p></td>
<td><p>Set the seed of the pseudo-random generators (python, numpy, pytorch, gym, action_space)</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#pantheonrl.algos.adap.adap_learn.ADAP.train" title="pantheonrl.algos.adap.adap_learn.ADAP.train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train</span></code></a></p></td>
<td><p>Update policy using the currently gathered rollout buffer.</p></td>
</tr>
</tbody>
</table>
<p class="rubric">Attributes</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#pantheonrl.algos.adap.adap_learn.ADAP.logger" title="pantheonrl.algos.adap.adap_learn.ADAP.logger"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logger</span></code></a></p></td>
<td><p>Getter for the logger object.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">policy_aliases</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">full_obs_shape</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">rollout_buffer</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">policy</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">observation_space</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">action_space</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">n_envs</span></code></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">lr_schedule</span></code></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="pantheonrl.algos.adap.adap_learn.ADAP.collect_rollouts">
<span class="sig-name descname"><span class="pre">collect_rollouts</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">env</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callback</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rollout_buffer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_rollout_steps</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pantheonrl/algos/adap/adap_learn.html#ADAP.collect_rollouts"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pantheonrl.algos.adap.adap_learn.ADAP.collect_rollouts" title="Link to this definition"></a></dt>
<dd><p>Nearly identical to OnPolicyAlgorithm’s collect_rollouts, but it also
resamples the context every episode.</p>
<p>Collect experiences using the current policy and fill a
<code class="docutils literal notranslate"><span class="pre">RolloutBuffer</span></code>.
The term rollout here refers to the model-free notion and should not
be used with the concept of rollout used in model-based RL or planning.
:param env: The training environment
:param callback: Callback that will be called at each step</p>
<blockquote>
<div><p>(and at the beginning and end of the rollout)</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rollout_buffer</strong> (<em>RolloutBuffer</em>) – Buffer to fill with rollouts</p></li>
<li><p><strong>n_steps</strong> – Number of experiences to collect per environment</p></li>
<li><p><strong>env</strong> (<em>VecEnv</em>) – </p></li>
<li><p><strong>callback</strong> (<em>BaseCallback</em>) – </p></li>
<li><p><strong>n_rollout_steps</strong> (<em>int</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>True if function returned with at least <cite>n_rollout_steps</cite>
collected, False if callback terminated rollout prematurely.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pantheonrl.algos.adap.adap_learn.ADAP.get_env">
<span class="sig-name descname"><span class="pre">get_env</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pantheonrl.algos.adap.adap_learn.ADAP.get_env" title="Link to this definition"></a></dt>
<dd><p>Returns the current environment (can be None if not defined).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The current environment</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><em>VecEnv</em> | None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pantheonrl.algos.adap.adap_learn.ADAP.get_parameters">
<span class="sig-name descname"><span class="pre">get_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pantheonrl.algos.adap.adap_learn.ADAP.get_parameters" title="Link to this definition"></a></dt>
<dd><p>Return the parameters of the agent. This includes parameters from different networks, e.g.
critics (value functions) and policies (pi functions).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Mapping of from names of the objects to PyTorch state-dicts.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><em>Dict</em>[str, <em>Dict</em>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pantheonrl.algos.adap.adap_learn.ADAP.get_vec_normalize_env">
<span class="sig-name descname"><span class="pre">get_vec_normalize_env</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pantheonrl.algos.adap.adap_learn.ADAP.get_vec_normalize_env" title="Link to this definition"></a></dt>
<dd><p>Return the <code class="docutils literal notranslate"><span class="pre">VecNormalize</span></code> wrapper of the training env
if it exists.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The <code class="docutils literal notranslate"><span class="pre">VecNormalize</span></code> env.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><em>VecNormalize</em> | None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pantheonrl.algos.adap.adap_learn.ADAP.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">total_timesteps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callback</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_interval</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_env</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_freq</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_eval_episodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tb_log_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ADAP'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_log_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reset_num_timesteps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pantheonrl/algos/adap/adap_learn.html#ADAP.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pantheonrl.algos.adap.adap_learn.ADAP.learn" title="Link to this definition"></a></dt>
<dd><p>Return a trained model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>total_timesteps</strong> (<em>int</em>) – The total number of samples (env steps) to train on</p></li>
<li><p><strong>callback</strong> (<em>None</em><em> | </em><em>Callable</em><em> | </em><em>List</em><em>[</em><em>BaseCallback</em><em>] </em><em>| </em><em>BaseCallback</em>) – callback(s) called at every step with state of the algorithm.</p></li>
<li><p><strong>log_interval</strong> (<em>int</em>) – The number of episodes before logging.</p></li>
<li><p><strong>tb_log_name</strong> (<em>str</em>) – the name of the run for TensorBoard logging</p></li>
<li><p><strong>reset_num_timesteps</strong> (<em>bool</em>) – whether or not to reset the current timestep number (used in logging)</p></li>
<li><p><strong>progress_bar</strong> – Display a progress bar using tqdm and rich.</p></li>
<li><p><strong>eval_env</strong> (<em>Env</em><em> | </em><em>VecEnv</em><em> | </em><em>None</em>) – </p></li>
<li><p><strong>eval_freq</strong> (<em>int</em>) – </p></li>
<li><p><strong>n_eval_episodes</strong> (<em>int</em>) – </p></li>
<li><p><strong>eval_log_path</strong> (<em>str</em><em> | </em><em>None</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>the trained model</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#pantheonrl.algos.adap.adap_learn.ADAP" title="pantheonrl.algos.adap.adap_learn.ADAP"><em>ADAP</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pantheonrl.algos.adap.adap_learn.ADAP.load">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_objects</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">print_system_info</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">force_reset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pantheonrl.algos.adap.adap_learn.ADAP.load" title="Link to this definition"></a></dt>
<dd><p>Load the model from a zip-file.
Warning: <code class="docutils literal notranslate"><span class="pre">load</span></code> re-creates the model from scratch, it does not update it in-place!
For an in-place load use <code class="docutils literal notranslate"><span class="pre">set_parameters</span></code> instead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> (<em>str</em><em> | </em><em>Path</em><em> | </em><em>BufferedIOBase</em>) – path to the file (or a file-like) where to
load the agent from</p></li>
<li><p><strong>env</strong> (<em>Env</em><em> | </em><em>VecEnv</em><em> | </em><em>None</em>) – the new environment to run the loaded model on
(can be None if you only need prediction from a trained model) has priority over any saved environment</p></li>
<li><p><strong>device</strong> (<em>device</em><em> | </em><em>str</em>) – Device on which the code should run.</p></li>
<li><p><strong>custom_objects</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>] </em><em>| </em><em>None</em>) – Dictionary of objects to replace
upon loading. If a variable is present in this dictionary as a
key, it will not be deserialized and the corresponding item
will be used instead. Similar to custom_objects in
<code class="docutils literal notranslate"><span class="pre">keras.models.load_model</span></code>. Useful when you have an object in
file that can not be deserialized.</p></li>
<li><p><strong>print_system_info</strong> (<em>bool</em>) – Whether to print system info from the saved model
and the current system info (useful to debug loading issues)</p></li>
<li><p><strong>force_reset</strong> (<em>bool</em>) – Force call to <code class="docutils literal notranslate"><span class="pre">reset()</span></code> before training
to avoid unexpected behavior.
See <a class="reference external" href="https://github.com/DLR-RM/stable-baselines3/issues/597">https://github.com/DLR-RM/stable-baselines3/issues/597</a></p></li>
<li><p><strong>kwargs</strong> – extra arguments to change the model when loading</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>new model instance with loaded parameters</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>SelfBaseAlgorithm</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="pantheonrl.algos.adap.adap_learn.ADAP.logger">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logger</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Logger</span></em><a class="headerlink" href="#pantheonrl.algos.adap.adap_learn.ADAP.logger" title="Link to this definition"></a></dt>
<dd><p>Getter for the logger object.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pantheonrl.algos.adap.adap_learn.ADAP.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">observation</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">episode_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">deterministic</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pantheonrl.algos.adap.adap_learn.ADAP.predict" title="Link to this definition"></a></dt>
<dd><p>Get the policy action from an observation (and optional hidden state).
Includes sugar-coating to handle different observations (e.g. normalizing images).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>observation</strong> (<em>ndarray</em><em> | </em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>ndarray</em><em>]</em>) – the input observation</p></li>
<li><p><strong>state</strong> (<em>Tuple</em><em>[</em><em>ndarray</em><em>, </em><em>...</em><em>] </em><em>| </em><em>None</em>) – The last hidden states (can be None, used in recurrent policies)</p></li>
<li><p><strong>episode_start</strong> (<em>ndarray</em><em> | </em><em>None</em>) – The last masks (can be None, used in recurrent policies)
this correspond to beginning of episodes,
where the hidden states of the RNN must be reset.</p></li>
<li><p><strong>deterministic</strong> (<em>bool</em>) – Whether or not to return deterministic actions.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>the model’s action and the next hidden state
(used in recurrent policies)</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>Tuple</em>[<em>ndarray</em>, <em>Tuple</em>[<em>ndarray</em>, …] | None]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pantheonrl.algos.adap.adap_learn.ADAP.save">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exclude</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pantheonrl.algos.adap.adap_learn.ADAP.save" title="Link to this definition"></a></dt>
<dd><p>Save all the attributes of the object and the model parameters in a zip-file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> (<em>str</em><em> | </em><em>Path</em><em> | </em><em>BufferedIOBase</em>) – path to the file where the rl agent should be saved</p></li>
<li><p><strong>exclude</strong> (<em>Iterable</em><em>[</em><em>str</em><em>] </em><em>| </em><em>None</em>) – name of parameters that should be excluded in addition to the default ones</p></li>
<li><p><strong>include</strong> (<em>Iterable</em><em>[</em><em>str</em><em>] </em><em>| </em><em>None</em>) – name of parameters that might be excluded but should be included anyway</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pantheonrl.algos.adap.adap_learn.ADAP.set_env">
<span class="sig-name descname"><span class="pre">set_env</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">env</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pantheonrl/algos/adap/adap_learn.html#ADAP.set_env"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pantheonrl.algos.adap.adap_learn.ADAP.set_env" title="Link to this definition"></a></dt>
<dd><p>Checks the validity of the environment, and if it is coherent, set it as the current environment.
Furthermore wrap any non vectorized env into a vectorized
checked parameters:
- observation_space
- action_space</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>env</strong> – The environment for learning a policy</p></li>
<li><p><strong>force_reset</strong> – Force call to <code class="docutils literal notranslate"><span class="pre">reset()</span></code> before training
to avoid unexpected behavior.
See issue <a class="reference external" href="https://github.com/DLR-RM/stable-baselines3/issues/597">https://github.com/DLR-RM/stable-baselines3/issues/597</a></p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pantheonrl.algos.adap.adap_learn.ADAP.set_logger">
<span class="sig-name descname"><span class="pre">set_logger</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logger</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pantheonrl.algos.adap.adap_learn.ADAP.set_logger" title="Link to this definition"></a></dt>
<dd><p>Setter for for logger object.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When passing a custom logger object,
this will overwrite <code class="docutils literal notranslate"><span class="pre">tensorboard_log</span></code> and <code class="docutils literal notranslate"><span class="pre">verbose</span></code> settings
passed to the constructor.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>logger</strong> (<em>Logger</em>) – </p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pantheonrl.algos.adap.adap_learn.ADAP.set_parameters">
<span class="sig-name descname"><span class="pre">set_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">load_path_or_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exact_match</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pantheonrl.algos.adap.adap_learn.ADAP.set_parameters" title="Link to this definition"></a></dt>
<dd><p>Load parameters from a given zip-file or a nested dictionary containing parameters for
different modules (see <code class="docutils literal notranslate"><span class="pre">get_parameters</span></code>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>load_path_or_iter</strong> – Location of the saved data (path or file-like, see <code class="docutils literal notranslate"><span class="pre">save</span></code>), or a nested
dictionary containing nn.Module parameters used by the policy. The dictionary maps
object names to a state-dictionary returned by <code class="docutils literal notranslate"><span class="pre">torch.nn.Module.state_dict()</span></code>.</p></li>
<li><p><strong>exact_match</strong> (<em>bool</em>) – If True, the given parameters should include parameters for each
module and each of their parameters, otherwise raises an Exception. If set to False, this
can be used to update only specific parameters.</p></li>
<li><p><strong>device</strong> (<em>device</em><em> | </em><em>str</em>) – Device on which the code should run.</p></li>
<li><p><strong>load_path_or_dict</strong> (<em>str</em><em> | </em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Tensor</em><em>]</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pantheonrl.algos.adap.adap_learn.ADAP.set_random_seed">
<span class="sig-name descname"><span class="pre">set_random_seed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pantheonrl.algos.adap.adap_learn.ADAP.set_random_seed" title="Link to this definition"></a></dt>
<dd><p>Set the seed of the pseudo-random generators
(python, numpy, pytorch, gym, action_space)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>seed</strong> (<em>int</em><em> | </em><em>None</em>) – </p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pantheonrl.algos.adap.adap_learn.ADAP.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pantheonrl/algos/adap/adap_learn.html#ADAP.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pantheonrl.algos.adap.adap_learn.ADAP.train" title="Link to this definition"></a></dt>
<dd><p>Update policy using the currently gathered rollout buffer.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="pantheonrl.algos.adap.adap_learn.html" class="btn btn-neutral float-left" title="pantheonrl.algos.adap.adap_learn" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="pantheonrl.algos.adap.agent.html" class="btn btn-neutral float-right" title="pantheonrl.algos.adap.agent" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Stanford ILIAD.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>